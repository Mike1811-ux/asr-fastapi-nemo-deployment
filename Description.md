# ðŸ“ Description.md â€” Hindi ASR API Assignment

## âœ… Features Implemented

- âœ”ï¸ Downloaded and used the official NVIDIA NeMo `stt_hi_conformer_ctc_medium` model for Hindi ASR.
- âœ”ï¸ Converted the model to ONNX format using TorchScript and `torch.onnx.export`.
- âœ”ï¸ Built a FastAPI server with a `/transcribe` endpoint.
- âœ”ï¸ Supported `.wav` audio files with strict validation:
  - Must be 16kHz sample rate.
  - Must be 5â€“10 seconds duration.
  - Only accepts `.wav` format.
- âœ”ï¸ Implemented model inference pipeline using ONNX Runtime.
- âœ”ï¸ Integrated audio preprocessing (resampling, normalization, mono conversion).
- âœ”ï¸ Created a clean Dockerfile using `python:3.10-slim` with minimal image size.
- âœ”ï¸ Wrote modular, well-commented, and production-ready Python code.
- âœ”ï¸ Provided detailed `README.md` and this description to support review.

---

## ðŸš§ Challenges Faced

### 1. **Model Optimization Complexity**
   - NeMo models are not natively ONNX-exportable due to dynamic graph structure.
   - Solution: Used TorchScript tracing as an intermediate step before ONNX export.

### 2. **Preprocessing Pipeline Matching**
   - The ONNX model expects log mel filterbank features with specific normalization.
   - Solution: Replicated the featurizer behavior using NeMoâ€™s `FilterbankFeatures` module.

### 3. **Audio Duration/Rate Handling**
   - Ensured rejection of invalid inputs at upload time using the `wave` module.
   - Auto-resampled any misaligned inputs to 16kHz.

---

## âŒ Limitations

- ðŸš« **True async inference** not implemented due to ONNXRuntimeâ€™s synchronous API. FastAPI supports async endpoints, but the inference call is blocking.
- ðŸš« **No automatic CI/CD or testing framework** included due to time constraints.

---

## ðŸ› ï¸ How Iâ€™d Improve Further

- âœ… Use a queue-based async inference architecture with Celery + Redis to support non-blocking ASR.
- âœ… Add pytest-based tests and GitHub Actions for CI.
- âœ… Build a frontend to upload audio and see real-time results.
- âœ… Use quantization-aware training or ONNX quantization tools to shrink the model size.

---

## â„¹ï¸ Assumptions

- The deployed model is inference-only and wonâ€™t be retrained.
- Input is Hindi speech only, and may fail for other languages.
- Model must be used in an environment with access to NVIDIAâ€™s pretrained weights (i.e., internet access for initial download).

---

## âœ… Summary

The project implements a fully functional ASR API with containerized deployment, clean modular code, strict validation, and documentation. All core objectives were met, and the system is designed for scalability and further enhancement.

> Thank you for reviewing this submission!
